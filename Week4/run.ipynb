{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "GAMMA = 0.98\n",
    "TAU = 0.01\n",
    "BATCHSIZE = 128\n",
    "N_BATCHES = 64\n",
    "N_MULTI_ENVS = 64\n",
    "EXPLORATION_RATE = 0.2\n",
    "EPSILON_DECAY = 0.999\n",
    "\n",
    "N_NEW_SAMPLES = 500\n",
    "N_SAMPLE_SETS = 100\n",
    "\n",
    "CHECKPOINT_FREQ = 40\n",
    "LOG_FREQ = 30\n",
    "N_TEST_ENVS = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_SAVE_TO_PATH = \"ckpts/ckpt\"\n",
    "TMP_LOG_PATH = \"logs/{}.json\"\n",
    "TB_LOGS = \"tb_lobs/run\"\n",
    "\n",
    "\n",
    "os.makedirs(TMP_SAVE_TO_PATH.replace(\"/ckpt\",\"\"), exist_ok= True)\n",
    "os.makedirs(TMP_LOG_PATH.replace(\"/{}.json\",\"\"), exist_ok= True)\n",
    "os.makedirs(TB_LOGS, exist_ok= True)\n",
    "\n",
    "# # get old checkpoint\n",
    "# !cp /content/gdrive/MyDrive/DeepRL/HW4/checkpoint.zip checkpoint.zip\n",
    "# !unzip -d {TMP_SAVE_TO_PATH} checkpoint.zip\n",
    "\n",
    "LOSS = tf.keras.losses.Huber()\n",
    "CNN_SHAPE = (84, 84)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def triple_conv_block_no_batchnorm(x, filters):\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters, 3, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters, 3, padding='same', activation='relu')(x) + x\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters, 3, padding='same', activation='relu')(x) + x\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_standard_dqn():\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    inputs = tf.keras.layers.Input(CNN_SHAPE + (3,))\n",
    "\n",
    "    x = triple_conv_block_no_batchnorm(inputs, 16)\n",
    "    x = tf.keras.layers.MaxPool2D(2)(x)\n",
    "    x = triple_conv_block_no_batchnorm(x, 32)\n",
    "    x = tf.keras.layers.MaxPool2D(2)(x)\n",
    "    x = triple_conv_block_no_batchnorm(x, 64)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(512, \"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(4, \"linear\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"standard_dqn\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_small_dqn():\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    inputs = tf.keras.layers.Input(CNN_SHAPE + (3,))\n",
    "    x = triple_conv_block_no_batchnorm(inputs, 10)\n",
    "    x = tf.keras.layers.MaxPool2D(2)(x)\n",
    "    x = triple_conv_block_no_batchnorm(x, 20)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(512, \"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(4, \"linear\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"standard_dqn\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def sample_trajectory(dqn, state, epsilon=0.2):\n",
    "\n",
    "    n_par = tf.shape(state)[0]\n",
    "\n",
    "    mask = tf.random.uniform((n_par,), 0, 1, tf.float32) > epsilon\n",
    "\n",
    "    predictions = dqn(state, training=False)\n",
    "    max_actions = tf.math.argmax(predictions, axis=-1)\n",
    "\n",
    "    random_choices = tf.random.uniform(\n",
    "        shape=[n_par], minval=0, maxval=4, dtype=tf.int64)\n",
    "\n",
    "    return tf.where(mask, max_actions, random_choices), tf.reduce_max(predictions, -1)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def preprocess_all(observation, next_observation, action, reward, terminated):\n",
    "\n",
    "    observation = tf.cast(observation, tf.float32)/255\n",
    "    observation = tf.image.resize(observation, CNN_SHAPE)\n",
    "\n",
    "    next_observation = tf.cast(next_observation, tf.float32)/255\n",
    "    next_observation = tf.image.resize(next_observation, CNN_SHAPE)\n",
    "\n",
    "    action = tf.cast(action, tf.int64)\n",
    "    reward = tf.cast(reward, tf.float32)\n",
    "    terminated = tf.cast(terminated, tf.bool)\n",
    "\n",
    "    return observation, next_observation, action, reward, terminated\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def preprocess_obersvation(observation):\n",
    "\n",
    "    observation = tf.cast(observation, tf.float32)/255\n",
    "\n",
    "    return tf.image.resize(observation, CNN_SHAPE)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def polyak_averaging(Q_target, Q_dqn, tau):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        Q_target (_type_): _description_\n",
    "        Q_dqn (_type_): _description_\n",
    "        tau (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    for old, new in zip(Q_target.trainable_variables, Q_dqn.trainable_variables):\n",
    "        update = old * (1 - tau) + new * tau\n",
    "        old.assign(update)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def update_q_network(data, dqn, q_target, optimizer, gamma):\n",
    "\n",
    "    state, next_state, action, reward, terminated = data\n",
    "\n",
    "    s_prime_values = q_target(next_state, training=False)\n",
    "    s_prime_values = tf.reduce_max(s_prime_values, -1)\n",
    "    mask = 1 - tf.cast(terminated, tf.float32)\n",
    "\n",
    "    labels = reward + gamma * mask * s_prime_values\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        predictions = dqn(state, training=True)\n",
    "        action_values = tf.gather(predictions, action, batch_dims=1)\n",
    "\n",
    "        loss = LOSS(action_values, labels)\n",
    "\n",
    "    gradients = tape.gradient(loss, dqn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, dqn.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "class ENV_SAMPLER:\n",
    "    \"\"\"\n",
    "    Class for sampling environment data using a DQN model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dqn, n_multi_envs) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the ENV_SAMPLER instance.\n",
    "\n",
    "        Args:\n",
    "            env: The environment to sample from.\n",
    "            dqn: The DQN model for action selection.\n",
    "            n_multi_envs: The number of parallel environments.\n",
    "            preprocess_observation: Function to preprocess observations.\n",
    "        \"\"\"\n",
    "        self.env = gym.vector.make('ALE/Breakout-v5', num_envs=n_multi_envs)\n",
    "        self.current_state = self.env.reset()[0]\n",
    "        self.dqn = dqn\n",
    "        self.n_multi_envs = n_multi_envs\n",
    "\n",
    "    def reset_env(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to the initial state.\n",
    "        \"\"\"\n",
    "        self.current_state = self.env.reset()[0]\n",
    "\n",
    "    def sample(self, n_samples, epsilon=0.2):\n",
    "        \"\"\"\n",
    "        Sample environment data.\n",
    "\n",
    "        Args:\n",
    "            n_samples: The number of samples to generate.\n",
    "            epsilon: The exploration factor for action selection (default: 0.2).\n",
    "\n",
    "        Returns:\n",
    "            samples: List of sampled data tuples (current_state, next_state, action, reward, terminated).\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "\n",
    "        n_steps = np.ceil(n_samples / self.n_multi_envs).astype(int)\n",
    "\n",
    "        for _ in range(n_steps):\n",
    "            oberservation_as_tensor = preprocess_obersvation(\n",
    "                self.current_state)\n",
    "\n",
    "            action, q_vals = map(lambda x: x.numpy(), sample_trajectory(\n",
    "                self.dqn, oberservation_as_tensor, epsilon))\n",
    "\n",
    "            observation, reward, terminated, truncated, info = self.env.step(\n",
    "                action)\n",
    "\n",
    "            for i in range(self.n_multi_envs):\n",
    "                samples.append((self.current_state[i],\n",
    "                                observation[i],\n",
    "                                action[i],\n",
    "                                reward[i],\n",
    "                                terminated[i]))\n",
    "\n",
    "            self.current_state = observation\n",
    "\n",
    "        return samples[:n_samples]\n",
    "\n",
    "    def measure_model_perforamnce(self, gamma: float, target_q, n_test_envs = 4):\n",
    "\n",
    "        test_env = gym.vector.make('ALE/Breakout-v5', num_envs=n_test_envs)\n",
    "\n",
    "        current_state = test_env.reset()[0]\n",
    "\n",
    "\n",
    "        rewards = np.zeros(self.n_multi_envs)\n",
    "        terminated_at = []\n",
    "        q_values = []\n",
    "        target_q_values = []\n",
    "\n",
    "        allready_terminated = np.zeros(self.n_multi_envs, np.bool)\n",
    "\n",
    "        steps = 0\n",
    "\n",
    "        while True:\n",
    "            \n",
    "\n",
    "            oberservation_as_tensor = preprocess_obersvation(\n",
    "                current_state)\n",
    "\n",
    "            action, q_vals = map(lambda x: x.numpy(), sample_trajectory(\n",
    "                self.dqn, oberservation_as_tensor, EXPLORATION_RATE))\n",
    "\n",
    "            target_vals = tf.reduce_max(target_q(oberservation_as_tensor), -1)\n",
    "\n",
    "            observation, reward, terminated, truncated, info = test_env.step(\n",
    "                action)\n",
    "\n",
    "            current_state = observation\n",
    "\n",
    "            rewards += (gamma ** steps) * reward * (1 - allready_terminated)\n",
    "\n",
    "\n",
    "            allready_terminated = np.logical_or(\n",
    "                allready_terminated, terminated)\n",
    "\n",
    "            for t in terminated:\n",
    "\n",
    "                if t:\n",
    "                    terminated_at.append(steps)\n",
    "\n",
    "            q_values.extend(q_vals.tolist())\n",
    "            target_q_values.extend(target_vals.numpy().tolist())\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if allready_terminated.all():\n",
    "\n",
    "                break\n",
    "\n",
    "        average_q_val = np.mean(q_values)\n",
    "        average_target_q_val = np.mean(target_q_values)\n",
    "\n",
    "        l2_diff = np.array(q_values) - np.array(target_q_values)\n",
    "        l2_diff = np.sqrt(np.square(l2_diff).mean())\n",
    "\n",
    "        average_rewards = np.mean(rewards)\n",
    "        average_termination = np.mean(terminated_at)\n",
    "        \n",
    "        test_env.close()\n",
    "\n",
    "        return average_rewards, average_termination, average_q_val, average_target_q_val, l2_diff\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Class for managing a replay buffer for reinforcement learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the ReplayBuffer instance.\n",
    "\n",
    "        Args:\n",
    "            preprocess_func: Function to preprocess examples.\n",
    "        \"\"\"\n",
    "        self.saved_trajectories = []\n",
    "\n",
    "    def add_new_trajectory(self, trajectory):\n",
    "        \"\"\"\n",
    "        Add a new trajectory to the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            trajectory: List of examples representing a trajectory.\n",
    "        \"\"\"\n",
    "        self.saved_trajectories.append(trajectory)\n",
    "\n",
    "    def drop_first_trajectory(self):\n",
    "        \"\"\"\n",
    "        Remove the oldest trajectory from the replay buffer.\n",
    "        \"\"\"\n",
    "        to_delete = self.saved_trajectories.pop(0)\n",
    "        del to_delete\n",
    "\n",
    "    def sample_singe_example(\n",
    "        self,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample a single example from the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            melt_stop_criteria: Boolean flag indicating whether to consider stop criteria (default: False).\n",
    "\n",
    "        Returns:\n",
    "            example: A single example from a randomly chosen trajectory.\n",
    "        \"\"\"\n",
    "        trajectory = random.choice(self.saved_trajectories)\n",
    "        example = random.choice(trajectory)\n",
    "\n",
    "        states, next_states, actions, rewards, terminations, = example\n",
    "\n",
    "        return states, next_states, actions, rewards, terminations\n",
    "\n",
    "    def sample_n_examples(self, n_examples: int):\n",
    "        \"\"\"\n",
    "        Sample multiple examples from the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            n_examples: The number of examples to sample.\n",
    "\n",
    "        Returns:\n",
    "            states, next_states, actions, rewards, stop_criteria: Arrays of sampled examples.\n",
    "        \"\"\"\n",
    "        trajectories = [self.sample_singe_example() for _ in range(n_examples)]\n",
    "\n",
    "        states, next_states, actions, rewards, stop_criteria = map(\n",
    "            np.array, zip(*trajectories)\n",
    "        )\n",
    "\n",
    "        return states, next_states, actions, rewards, stop_criteria\n",
    "\n",
    "    def generate_tf_dataset(self, n_batches, batchsize):\n",
    "        \"\"\"\n",
    "        Generate a TensorFlow dataset from the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            n_batches: The number of batches to generate.\n",
    "            batchsize: The size of each batch.\n",
    "\n",
    "        Returns:\n",
    "            ds: TensorFlow dataset containing the preprocessed examples.\n",
    "        \"\"\"\n",
    "        n_steps = n_batches * batchsize\n",
    "\n",
    "        ds = self.sample_n_examples(n_steps)\n",
    "        ds = tf.data.Dataset.from_tensor_slices(ds)\n",
    "        ds = ds.map(preprocess_all, tf.data.AUTOTUNE)\n",
    "        ds = ds.batch(batchsize)\n",
    "\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 15:55:43.608332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 15:55:43.612807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 15:55:43.612970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 15:55:43.613248: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 15:55:43.613620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 15:55:43.613757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 15:55:43.613866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 15:55:44.015082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 15:55:44.015278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 15:55:44.015403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 15:55:44.015511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6738 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "q_net = get_standard_dqn()\n",
    "\n",
    "target_net = tf.keras.models.clone_model(q_net)\n",
    "\n",
    "env_sampler = ENV_SAMPLER(q_net, N_MULTI_ENVS)\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(q_net = q_net, target_net = target_net, )#optimizer = optimizer)\n",
    "\n",
    "writer = tf.summary.create_file_writer(TB_LOGS)\n",
    "\n",
    "# restore\n",
    "# checkpoint.restore(TMP_SAVE_TO_PATH\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb082daf4f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint.restore(TMP_SAVE_TO_PATH + \"-68\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fill Buffer initally:   0%|          | 0/100 [00:00<?, ?it/s]2023-06-14 15:54:52.946197: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n",
      "2023-06-14 15:54:53.148340: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-06-14 15:54:53.148835: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-06-14 15:54:53.148849: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2023-06-14 15:54:53.149385: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-06-14 15:54:53.149439: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "Fill Buffer initally: 100%|██████████| 100/100 [00:23<00:00,  4.27it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm.tqdm(range(N_SAMPLE_SETS), desc = \"Fill Buffer initally\"):\n",
    "\n",
    "    new_samples = env_sampler.sample(N_NEW_SAMPLES, epsilon = 1)\n",
    "    replay_buffer.add_new_trajectory(new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_steps = 0\n",
    "\n",
    "env_sampler.reset_env()\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "\n",
    "    outer_steps +=1\n",
    "    EXPLORATION_RATE *= EPSILON_DECAY\n",
    "    \n",
    "    ds = replay_buffer.generate_tf_dataset(N_BATCHES, BATCHSIZE)\n",
    "    \n",
    "    bar = ds # tqdm.tqdm(ds)\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    for data in bar:\n",
    "\n",
    "        loss = update_q_network(data, q_net, target_net, optimizer, GAMMA)\n",
    "        \n",
    "        \n",
    "        losses.append(loss)\n",
    "        mean_loss = float(np.mean(losses))\n",
    "        # bar.set_description(f\"Loss {mean_loss:.6f}\")\n",
    "\n",
    "    mean_loss = float(np.mean(losses))\n",
    "    \n",
    "    with writer.as_default():\n",
    "\n",
    "        tf.summary.scalar(\"loss\", mean_loss, step=outer_steps)\n",
    "        tf.summary.scalar(\"epsilon\", EXPLORATION_RATE, step=outer_steps)\n",
    "\n",
    "\n",
    "    new_samples = env_sampler.sample(N_NEW_SAMPLES, epsilon= EXPLORATION_RATE)\n",
    "    replay_buffer.add_new_trajectory(new_samples)\n",
    "\n",
    "    replay_buffer.drop_first_trajectory()\n",
    "\n",
    "    polyak_averaging(target_net, q_net, TAU)\n",
    "    \n",
    "    \n",
    "    if outer_steps % LOG_FREQ == 0:\n",
    "        \n",
    "        print(\"Logging Model Metrics\")\n",
    "        \n",
    "        results = env_sampler.measure_model_perforamnce( GAMMA, target_net, N_TEST_ENVS)\n",
    "        names = [\"average_rewards\", \"average_termination\", \"average_q_val\", \"average_target_q_val\", \"l2_diff\",]\n",
    "\n",
    "        for val, name in zip(results, names):\n",
    "\n",
    "            with writer.as_default():\n",
    "\n",
    "                tf.summary.scalar(name, val, step=outer_steps)\n",
    "\n",
    "        results = pd.Series(results, names)\n",
    "        \n",
    "        results[\"average_loss\"] = np.mean(losses)\n",
    "        \n",
    "        results.to_json(TMP_LOG_PATH.format(outer_steps))\n",
    "        \n",
    "        print(results)\n",
    "        \n",
    "        \n",
    "    if outer_steps % CHECKPOINT_FREQ == 0:\n",
    "        \n",
    "        print(\"Saving Checkpoint\")\n",
    "        \n",
    "        checkpoint.save(TMP_SAVE_TO_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
