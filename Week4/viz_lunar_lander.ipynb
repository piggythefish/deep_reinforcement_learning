{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "GAMMA = 0.99\n",
    "TAU = 0.01\n",
    "BATCHSIZE = 128\n",
    "N_BATCHES = 64\n",
    "N_MULTI_ENVS = 64\n",
    "EXPLORATION_RATE = 0.75\n",
    "EPSILON_DECAY = 0.9993\n",
    "\n",
    "N_NEW_SAMPLES = 500\n",
    "N_SAMPLE_SETS = 100\n",
    "\n",
    "CHECKPOINT_FREQ = 100\n",
    "LOG_FREQ = 100\n",
    "N_TEST_ENVS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_SAVE_TO_PATH = \"ckpts/ckpt\"\n",
    "TMP_LOG_PATH = \"logs/{}.json\"\n",
    "TB_LOGS = \"tb_lobs/run\"\n",
    "\n",
    "\n",
    "os.makedirs(TMP_SAVE_TO_PATH.replace(\"/ckpt\",\"\"), exist_ok= True)\n",
    "os.makedirs(TMP_LOG_PATH.replace(\"/{}.json\",\"\"), exist_ok= True)\n",
    "os.makedirs(TB_LOGS, exist_ok= True)\n",
    "\n",
    "# # get old checkpoint\n",
    "# !cp /content/gdrive/MyDrive/DeepRL/HW4/checkpoint.zip checkpoint.zip\n",
    "# !unzip -d {TMP_SAVE_TO_PATH} checkpoint.zip\n",
    "\n",
    "LOSS = tf.keras.losses.Huber()\n",
    "CNN_SHAPE = (84, 84)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def triple_conv_block_no_batchnorm(x, filters):\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters, 3, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters, 3, padding='same', activation='relu')(x) + x\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters, 3, padding='same', activation='relu')(x) + x\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_standard_dqn():\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    inputs = tf.keras.Input((8,))\n",
    "    x = tf.keras.layers.Dense(512, \"relu\")(inputs)\n",
    "    x = tf.keras.layers.Dropout(0.2,)(x)\n",
    "    x = tf.keras.layers.Dense(512, \"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.2,)(x)\n",
    "    x = tf.keras.layers.Dense(512, \"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(4, \"linear\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"standard_dqn\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def sample_trajectory(dqn, state, epsilon=0.2):\n",
    "\n",
    "    n_par = tf.shape(state)[0]\n",
    "\n",
    "    mask = tf.random.uniform((n_par,), 0, 1, tf.float32) > epsilon\n",
    "\n",
    "    predictions = dqn(state, training=False)\n",
    "    max_actions = tf.math.argmax(predictions, axis=-1)\n",
    "\n",
    "    random_choices = tf.random.uniform(\n",
    "        shape=[n_par], minval=0, maxval=4, dtype=tf.int64)\n",
    "\n",
    "    return tf.where(mask, max_actions, random_choices), tf.reduce_max(predictions, -1)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def preprocess_all(observation, next_observation, action, reward, terminated):\n",
    "\n",
    "    observation = tf.cast(observation, tf.float32)\n",
    "\n",
    "    next_observation = tf.cast(next_observation, tf.float32)\n",
    "\n",
    "    action = tf.cast(action, tf.int64)\n",
    "    reward = tf.cast(reward, tf.float32)\n",
    "    terminated = tf.cast(terminated, tf.bool)\n",
    "\n",
    "    return observation, next_observation, action, reward, terminated\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def preprocess_obersvation(observation):\n",
    "\n",
    "    observation = tf.cast(observation, tf.float32)\n",
    "\n",
    "    return observation\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def polyak_averaging(Q_target, Q_dqn, tau):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        Q_target (_type_): _description_\n",
    "        Q_dqn (_type_): _description_\n",
    "        tau (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    for old, new in zip(Q_target.trainable_variables, Q_dqn.trainable_variables):\n",
    "        update = old * (1 - tau) + new * tau\n",
    "        old.assign(update)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def update_q_network(data, dqn, q_target, optimizer, gamma):\n",
    "\n",
    "    state, next_state, action, reward, terminated = data\n",
    "\n",
    "    s_prime_values = q_target(next_state, training=False)\n",
    "    s_prime_values = tf.reduce_max(s_prime_values, -1)\n",
    "    \n",
    "    labels = reward + gamma * s_prime_values * (1 - tf.cast(terminated, tf.float32))\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        predictions = dqn(state, training=True)\n",
    "        action_values = tf.gather(predictions, action, batch_dims=1)\n",
    "\n",
    "        loss = LOSS(action_values, labels)\n",
    "\n",
    "    gradients = tape.gradient(loss, dqn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, dqn.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "class ENV_SAMPLER:\n",
    "    \"\"\"\n",
    "    Class for sampling environment data using a DQN model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dqn, n_multi_envs) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the ENV_SAMPLER instance.\n",
    "\n",
    "        Args:\n",
    "            env: The environment to sample from.\n",
    "            dqn: The DQN model for action selection.\n",
    "            n_multi_envs: The number of parallel environments.\n",
    "            preprocess_observation: Function to preprocess observations.\n",
    "        \"\"\"\n",
    "        self.env = gym.vector.make(\"LunarLander-v2\", num_envs=n_multi_envs)\n",
    "        self.current_state = self.env.reset()[0]\n",
    "        self.dqn = dqn\n",
    "        self.n_multi_envs = n_multi_envs\n",
    "\n",
    "    def reset_env(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to the initial state.\n",
    "        \"\"\"\n",
    "        self.current_state = self.env.reset()[0]\n",
    "\n",
    "    def sample(self, n_samples, epsilon=0.2):\n",
    "        \"\"\"\n",
    "        Sample environment data.\n",
    "\n",
    "        Args:\n",
    "            n_samples: The number of samples to generate.\n",
    "            epsilon: The exploration factor for action selection (default: 0.2).\n",
    "\n",
    "        Returns:\n",
    "            samples: List of sampled data tuples (current_state, next_state, action, reward, terminated).\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "\n",
    "        n_steps = np.ceil(n_samples / self.n_multi_envs).astype(int)\n",
    "\n",
    "        for _ in range(n_steps):\n",
    "            oberservation_as_tensor = preprocess_obersvation(\n",
    "                self.current_state)\n",
    "\n",
    "            action, q_vals = map(lambda x: x.numpy(), sample_trajectory(\n",
    "                self.dqn, oberservation_as_tensor, epsilon))\n",
    "\n",
    "            observation, reward, terminated, truncated, info = self.env.step(\n",
    "                action)\n",
    "\n",
    "            for i in range(self.n_multi_envs):\n",
    "                samples.append((self.current_state[i],\n",
    "                                observation[i],\n",
    "                                action[i],\n",
    "                                reward[i],\n",
    "                                terminated[i]))\n",
    "\n",
    "            self.current_state = observation\n",
    "\n",
    "        return samples[:n_samples]\n",
    "\n",
    "    def measure_model_perforamnce(self, gamma: float, target_q, n_test_envs = 4):\n",
    "\n",
    "        test_env = gym.vector.make(\"LunarLander-v2\", num_envs=n_test_envs)\n",
    "\n",
    "        current_state = test_env.reset()[0]\n",
    "\n",
    "\n",
    "        rewards = np.zeros(n_test_envs)\n",
    "        terminated_at = []\n",
    "        q_values = []\n",
    "        target_q_values = []\n",
    "\n",
    "        allready_terminated = np.zeros(n_test_envs, bool)\n",
    "\n",
    "        steps = 0\n",
    "\n",
    "        while True:\n",
    "            \n",
    "\n",
    "            oberservation_as_tensor = preprocess_obersvation(\n",
    "                current_state)\n",
    "\n",
    "            action, q_vals = map(lambda x: x.numpy(), sample_trajectory(\n",
    "                self.dqn, oberservation_as_tensor, 0.05))\n",
    "\n",
    "            target_vals = tf.reduce_max(target_q(oberservation_as_tensor), -1)\n",
    "\n",
    "            observation, reward, terminated, truncated, info = test_env.step(\n",
    "                action)\n",
    "\n",
    "            current_state = observation\n",
    "\n",
    "            rewards += (gamma ** steps) * reward * (1 - allready_terminated)\n",
    "\n",
    "\n",
    "            allready_terminated = np.logical_or(\n",
    "                allready_terminated, terminated)\n",
    "\n",
    "            for index,t in enumerate(terminated):\n",
    "\n",
    "                if t:\n",
    "                    terminated_at.append(steps)\n",
    "\n",
    "            q_values.extend(q_vals.tolist())\n",
    "            target_q_values.extend(target_vals.numpy().tolist())\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if allready_terminated.all():\n",
    "\n",
    "                break\n",
    "\n",
    "        average_q_val = np.mean(q_values)\n",
    "        average_target_q_val = np.mean(target_q_values)\n",
    "\n",
    "        l2_diff = np.array(q_values) - np.array(target_q_values)\n",
    "        l2_diff = np.sqrt(np.square(l2_diff).mean())\n",
    "\n",
    "        average_rewards = np.mean(rewards)\n",
    "        average_termination = np.mean(terminated_at)\n",
    "        \n",
    "        test_env.close()\n",
    "\n",
    "        return average_rewards, average_termination, average_q_val, average_target_q_val, l2_diff\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Class for managing a replay buffer for reinforcement learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the ReplayBuffer instance.\n",
    "\n",
    "        Args:\n",
    "            preprocess_func: Function to preprocess examples.\n",
    "        \"\"\"\n",
    "        self.saved_trajectories = []\n",
    "\n",
    "    def add_new_trajectory(self, trajectory):\n",
    "        \"\"\"\n",
    "        Add a new trajectory to the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            trajectory: List of examples representing a trajectory.\n",
    "        \"\"\"\n",
    "        self.saved_trajectories.append(trajectory)\n",
    "\n",
    "    def drop_first_trajectory(self):\n",
    "        \"\"\"\n",
    "        Remove the oldest trajectory from the replay buffer.\n",
    "        \"\"\"\n",
    "        to_delete = self.saved_trajectories.pop(0)\n",
    "        del to_delete\n",
    "\n",
    "    def sample_singe_example(\n",
    "        self,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample a single example from the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            melt_stop_criteria: Boolean flag indicating whether to consider stop criteria (default: False).\n",
    "\n",
    "        Returns:\n",
    "            example: A single example from a randomly chosen trajectory.\n",
    "        \"\"\"\n",
    "        trajectory = random.choice(self.saved_trajectories)\n",
    "        example = random.choice(trajectory)\n",
    "\n",
    "        states, next_states, actions, rewards, terminations, = example\n",
    "\n",
    "        return states, next_states, actions, rewards, terminations\n",
    "\n",
    "    def sample_n_examples(self, n_examples: int):\n",
    "        \"\"\"\n",
    "        Sample multiple examples from the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            n_examples: The number of examples to sample.\n",
    "\n",
    "        Returns:\n",
    "            states, next_states, actions, rewards, stop_criteria: Arrays of sampled examples.\n",
    "        \"\"\"\n",
    "        trajectories = [self.sample_singe_example() for _ in range(n_examples)]\n",
    "\n",
    "        states, next_states, actions, rewards, stop_criteria = map(\n",
    "            np.array, zip(*trajectories)\n",
    "        )\n",
    "\n",
    "        return states, next_states, actions, rewards, stop_criteria\n",
    "\n",
    "    def generate_tf_dataset(self, n_batches, batchsize):\n",
    "        \"\"\"\n",
    "        Generate a TensorFlow dataset from the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            n_batches: The number of batches to generate.\n",
    "            batchsize: The size of each batch.\n",
    "\n",
    "        Returns:\n",
    "            ds: TensorFlow dataset containing the preprocessed examples.\n",
    "        \"\"\"\n",
    "        n_steps = n_batches * batchsize\n",
    "\n",
    "        ds = self.sample_n_examples(n_steps)\n",
    "        ds = tf.data.Dataset.from_tensor_slices(ds)\n",
    "        ds = ds.map(preprocess_all, tf.data.AUTOTUNE)\n",
    "        ds = ds.batch(batchsize)\n",
    "\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f47efc9e830>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "q_net = get_standard_dqn()\n",
    "\n",
    "target_net = tf.keras.models.clone_model(q_net)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(q_net = q_net, target_net = target_net, optimizer = optimizer)\n",
    "\n",
    "writer = tf.summary.create_file_writer(TB_LOGS)\n",
    "\n",
    "checkpoint.restore(\"ckpts/ckpt-27\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"LunarLander-v2\",render_mode=\"rgb_array\")\n",
    "\n",
    "current_state = env.reset()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [env.render()]\n",
    "rewards = [0]\n",
    "\n",
    "while True:\n",
    "    \n",
    "    state_tensor = preprocess_obersvation(current_state)[None,:]\n",
    "    \n",
    "    action = sample_trajectory(q_net, state_tensor, 0.0)[0][0].numpy()\n",
    "    \n",
    "    observation, reward, terminated, truncated, info = env.step(\n",
    "                action)\n",
    "\n",
    "    \n",
    "    frames.append(env.render())\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    current_state = observation\n",
    "    \n",
    "    if terminated:\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "762"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "steps = np.arange(len(frames))\n",
    "\n",
    "im = plt.imshow(frames[0])\n",
    "plt.axis('off')\n",
    "\n",
    "def update(i):\n",
    "    \n",
    "    # ttl.set_text(\n",
    "    #     f\"Current Reward: {rewards[rounded]:.2f}\",\n",
    "    # )\n",
    "\n",
    "    im.set_array(frames[i])\n",
    "    return (im,)\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, steps)\n",
    "\n",
    "ani.save(\"lunar_lander.mp4\", savefig_kwargs={\"facecolor\": (1, 1, 1, 1)}, fps = 30)\n",
    "ani.save(\"lunar_lander.gif\", savefig_kwargs={\"facecolor\": (1, 1, 1, 1)}, fps = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126.49901957092042"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
